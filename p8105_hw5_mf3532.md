p8105_hw5_mf3532
================
Meng Fang
2022-11-03

## Problem 2

#### Load Data and Describe Dataset

``` r
homicide <-  read_csv("./homicide-data.csv")
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
head(homicide)
```

    ## # A tibble: 6 × 12
    ##   uid    repor…¹ victi…² victi…³ victi…⁴ victi…⁵ victi…⁶ city  state   lat   lon
    ##   <chr>    <dbl> <chr>   <chr>   <chr>   <chr>   <chr>   <chr> <chr> <dbl> <dbl>
    ## 1 Alb-0…  2.01e7 GARCIA  JUAN    Hispan… 78      Male    Albu… NM     35.1 -107.
    ## 2 Alb-0…  2.01e7 MONTOYA CAMERON Hispan… 17      Male    Albu… NM     35.1 -107.
    ## 3 Alb-0…  2.01e7 SATTER… VIVIANA White   15      Female  Albu… NM     35.1 -107.
    ## 4 Alb-0…  2.01e7 MENDIO… CARLOS  Hispan… 32      Male    Albu… NM     35.1 -107.
    ## 5 Alb-0…  2.01e7 MULA    VIVIAN  White   72      Female  Albu… NM     35.1 -107.
    ## 6 Alb-0…  2.01e7 BOOK    GERALD… White   91      Female  Albu… NM     35.2 -107.
    ## # … with 1 more variable: disposition <chr>, and abbreviated variable names
    ## #   ¹​reported_date, ²​victim_last, ³​victim_first, ⁴​victim_race, ⁵​victim_age,
    ## #   ⁶​victim_sex

The homicide dataset describes homicides in 50 large U.S. cities. This
dataset have 52179 rows, and 12 columns, there are 12 variables
involving in the dataset, and those variables describes the case id,
reported date, information of the victim(name, age, race, sex), city and
state the case happened, the latitude and longtitude of the place that
the case happened and the disposition status of the case.

#### Create a city_state variable and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides.

``` r
homicide <- homicide %>%
  mutate(city_state = str_c(city,",",state),
         status = case_when(
           disposition == "Closed without arrest" ~ "unsolved",
           disposition == "Open/No arrest" ~ "unsolved",
           disposition == "Closed by arrest" ~ "solved")) %>% 
  group_by(city_state, status) %>% 
  summarise(count = n()) %>%
  pivot_wider(names_from = status, values_from = count) %>% mutate(total = solved + unsolved) %>% 
  filter(city_state != "Tulsa,AL") %>%
  select(-solved)
```

    ## `summarise()` has grouped output by 'city_state'. You can override using the
    ## `.groups` argument.

``` r
knitr::kable(homicide)
```

| city_state        | unsolved | total |
|:------------------|---------:|------:|
| Albuquerque,NM    |      146 |   378 |
| Atlanta,GA        |      373 |   973 |
| Baltimore,MD      |     1825 |  2827 |
| Baton Rouge,LA    |      196 |   424 |
| Birmingham,AL     |      347 |   800 |
| Boston,MA         |      310 |   614 |
| Buffalo,NY        |      319 |   521 |
| Charlotte,NC      |      206 |   687 |
| Chicago,IL        |     4073 |  5535 |
| Cincinnati,OH     |      309 |   694 |
| Columbus,OH       |      575 |  1084 |
| Dallas,TX         |      754 |  1567 |
| Denver,CO         |      169 |   312 |
| Detroit,MI        |     1482 |  2519 |
| Durham,NC         |      101 |   276 |
| Fort Worth,TX     |      255 |   549 |
| Fresno,CA         |      169 |   487 |
| Houston,TX        |     1493 |  2942 |
| Indianapolis,IN   |      594 |  1322 |
| Jacksonville,FL   |      597 |  1168 |
| Kansas City,MO    |      486 |  1190 |
| Las Vegas,NV      |      572 |  1381 |
| Long Beach,CA     |      156 |   378 |
| Los Angeles,CA    |     1106 |  2257 |
| Louisville,KY     |      261 |   576 |
| Memphis,TN        |      483 |  1514 |
| Miami,FL          |      450 |   744 |
| Milwaukee,wI      |      403 |  1115 |
| Minneapolis,MN    |      187 |   366 |
| Nashville,TN      |      278 |   767 |
| New Orleans,LA    |      930 |  1434 |
| New York,NY       |      243 |   627 |
| Oakland,CA        |      508 |   947 |
| Oklahoma City,OK  |      326 |   672 |
| Omaha,NE          |      169 |   409 |
| Philadelphia,PA   |     1360 |  3037 |
| Phoenix,AZ        |      504 |   914 |
| Pittsburgh,PA     |      337 |   631 |
| Richmond,VA       |      113 |   429 |
| Sacramento,CA     |      139 |   376 |
| San Antonio,TX    |      357 |   833 |
| San Bernardino,CA |      170 |   275 |
| San Diego,CA      |      175 |   461 |
| San Francisco,CA  |      336 |   663 |
| Savannah,GA       |      115 |   246 |
| St. Louis,MO      |      905 |  1677 |
| Stockton,CA       |      266 |   444 |
| Tampa,FL          |       95 |   208 |
| Tulsa,OK          |      193 |   583 |
| Washington,DC     |      589 |  1345 |

#### For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object

``` r
baltimore_df <- homicide %>% filter(city_state == "Baltimore,MD")
```

plug in corresponding variables to `prop.test` function.

``` r
prop.test(baltimore_df%>%pull(unsolved), baltimore_df%>%pull(total)) %>%
  tidy() %>% 
  select(estimate, conf.low, conf.high)
```

    ## # A tibble: 1 × 3
    ##   estimate conf.low conf.high
    ##      <dbl>    <dbl>     <dbl>
    ## 1    0.646    0.628     0.663

#### Run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each, and create a tidy dataframe with estimated proportions and CIs for each city.

-   first create a function to get estimate, and bounds for CIs.

``` r
prop_ci <- function(x, n){
  prop.test(x,n) %>% tidy() %>% select(estimate, conf.low, conf.high)
}
```

-   Get the result dataframe

``` r
result_df <- homicide %>%
  mutate(prop_tests = map2(.x = unsolved, .y = total, ~prop_ci(x = .x, n = .y))) %>%
  unnest() %>%
  select(estimate, conf.low, conf.high)
```

    ## Warning: `cols` is now required when using unnest().
    ## Please use `cols = c(prop_tests)`

    ## Adding missing grouping variables: `city_state`

``` r
result_df
```

    ## # A tibble: 50 × 4
    ## # Groups:   city_state [50]
    ##    city_state     estimate conf.low conf.high
    ##    <chr>             <dbl>    <dbl>     <dbl>
    ##  1 Albuquerque,NM    0.386    0.337     0.438
    ##  2 Atlanta,GA        0.383    0.353     0.415
    ##  3 Baltimore,MD      0.646    0.628     0.663
    ##  4 Baton Rouge,LA    0.462    0.414     0.511
    ##  5 Birmingham,AL     0.434    0.399     0.469
    ##  6 Boston,MA         0.505    0.465     0.545
    ##  7 Buffalo,NY        0.612    0.569     0.654
    ##  8 Charlotte,NC      0.300    0.266     0.336
    ##  9 Chicago,IL        0.736    0.724     0.747
    ## 10 Cincinnati,OH     0.445    0.408     0.483
    ## # … with 40 more rows

#### Create a plot that shows the estimates and CIs for each city

``` r
result_df %>%
  ggplot(aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  xlab("City and State") +
  ylab("Proportion of Unsolved Murders") +
  ggtitle("Estimated Proportion and CIs of Unsolved Murder Cases ")
```

![](p8105_hw5_mf3532_files/figure-gfm/unnamed-chunk-8-1.png)<!-- -->

## Problem 3

#### Write a function to generate 5000 datasets from the model, while at the same time save mu hat and p-value.

``` r
sim_stat <- function(mu, sigma, n){
  output <- vector("list", 5000)
  i = 1
  while(i <= 5000){
    sim_data = rnorm(n, mean = mu, sd = sigma)
    
    sim_stats = t.test(sim_data, mu = 0, conf.level = 0.95) %>% 
    tidy() %>%
    select(estimate, p.value)
    
    output[[i]] = sim_stats
    
    i = i+1
  }

  bind_rows(output)
}
```

#### Generate 5000 datasets from the model mu = 0, sigma = 5, n = 30, and get the estimate and stats from each test

``` r
sim_stat(0,5,30)
```

    ## # A tibble: 5,000 × 2
    ##    estimate p.value
    ##       <dbl>   <dbl>
    ##  1   -1.69   0.0704
    ##  2    0.552  0.463 
    ##  3    0.413  0.483 
    ##  4    0.354  0.715 
    ##  5    0.785  0.324 
    ##  6   -0.646  0.400 
    ##  7   -0.914  0.318 
    ##  8    0.479  0.566 
    ##  9   -0.190  0.835 
    ## 10   -0.974  0.274 
    ## # … with 4,990 more rows

#### Repeat the above for $\mu = \{1,2,3,4,5,6\}$, and we make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of mu hat on the x axis.

``` r
power_df <- data.frame()

for(i in 0:6) {
  power_df[i+1,1] = i
  power_df[i+1,2] = pull(sim_stat(i,5,30) %>% filter(p.value <= 0.05) %>% count()/5000)
}
```

#### Make a plot of power and the true value of mu.

``` r
power_df %>% transmute(true_mu = V1, power = V2) %>%
  mutate(true_mu = as.numeric(true_mu)) %>%
  ggplot(aes(x = true_mu, y = power)) +
  geom_point(aes(color = true_mu, size = 3)) +
  geom_smooth(se = FALSE) + 
  labs(
    x = "True Mu",
    y = "Power",
    title = "True Mu vs Power"
  )
```

    ## `geom_smooth()` using method = 'loess' and formula 'y ~ x'

![](p8105_hw5_mf3532_files/figure-gfm/unnamed-chunk-12-1.png)<!-- -->

From the plot, we can see that when the effect size increases, the power
will increase and will be eventually close to 1.

#### Make a plot showing the average estimate of $\hat{\mu}$ on the y axis and the true value of $\mu$ on the x axis and ake a second plot (or overlay on the first) the average estimate of $\hat{\mu}$ only in samples for which the null was rejected on the y axis and the true value of $\mu$ on the x axis.

``` r
average_df <- data.frame()

for(i in 0:6) {
  average_df[i+1,1] = i
  average_df[i+1,2] = sim_stat(i,5,30) %>% select(estimate) %>% colMeans()
  average_df[i+1,3] = sim_stat(i,5,30) %>% filter(p.value <= 0.05) %>% select(estimate) %>% colMeans()
}
```

``` r
average_df %>%
  transmute(true_mu = V1, estimate_mean = V2, reject_mean = V3) %>%
  pivot_longer(2:3, values_to = "mean_value", names_to = "mean_type") %>%
  ggplot() +
  geom_point(mapping = aes(x = true_mu, y = mean_value, color = mean_type)) +
  geom_smooth(mapping = aes(x = true_mu, y = mean_value, color = mean_type), se = FALSE) +
  labs(
    x = "True Mu",
    y = "Estimate of Mu",
    title = "True Mu vs Power"
  )
```

    ## `geom_smooth()` using method = 'loess' and formula 'y ~ x'

![](p8105_hw5_mf3532_files/figure-gfm/unnamed-chunk-14-1.png)<!-- -->

The true mean is approximately equal to the average estimate for all
samples. However, in the plot of mean estimate for the rejected sample,
the average estimate only approximates the true mean when the effect
size is between 4 and 6. This shows that when effect size is large, the
average estimate is a good approximation of the true value, and when the
effect size is small, the the average estimate is not a good
approximation of the true value.
