---
title: "p8105_hw5_mf3532"
author: "Meng Fang"
date: '2022-11-03'
output: github_document
---


```{r, include = FALSE}
library(tidyverse)
library(stringr)
library(broom)
```

## Problem 2

#### Load Data and Describe Dataset

```{r}
homicide <-  read_csv("./homicide-data.csv")
head(homicide)
```

The homicide dataset describes homicides in 50 large U.S. cities. This dataset have `r nrow(homicide)` rows, and `r ncol(homicide)` columns, there are 12 variables involving in the dataset, and those variables describes the case id, reported date, information of the victim(name, age, race, sex), city and state the case happened, the latitude and longtitude of the place that the case happened and the disposition status of the case.


#### Create a city_state variable and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides.

```{r}
homicide <- homicide %>%
  mutate(city_state = str_c(city,",",state),
         status = case_when(
           disposition == "Closed without arrest" ~ "unsolved",
           disposition == "Open/No arrest" ~ "unsolved",
           disposition == "Closed by arrest" ~ "solved")) %>% 
  group_by(city_state, status) %>% 
  summarise(count = n()) %>%
  pivot_wider(names_from = status, values_from = count) %>% mutate(total = solved + unsolved) %>% 
  filter(city_state != "Tulsa,AL") %>%
  select(-solved)

knitr::kable(homicide)
```


#### For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object

```{r}
baltimore_df <- homicide %>% filter(city_state == "Baltimore,MD")
```

plug in corresponding variables to `prop.test` function.

```{r}
prop.test(baltimore_df%>%pull(unsolved), baltimore_df%>%pull(total)) %>%
  tidy() %>% 
  select(estimate, conf.low, conf.high)
```

#### Run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each, and create a tidy dataframe with estimated proportions and CIs for each city.

- first create a function to get estimate, and bounds for CIs.

```{r}
prop_ci <- function(x, n){
  prop.test(x,n) %>% tidy() %>% select(estimate, conf.low, conf.high)
}
```

- Get the result dataframe

```{r}
result_df <- homicide %>%
  mutate(prop_tests = map2(.x = unsolved, .y = total, ~prop_ci(x = .x, n = .y))) %>%
  unnest() %>%
  select(estimate, conf.low, conf.high)
  
result_df
```

#### Create a plot that shows the estimates and CIs for each city

```{r}
result_df %>%
  ggplot(aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  xlab("City and State") +
  ylab("Proportion of Unsolved Murders") +
  ggtitle("Estimated Proportion and CIs of Unsolved Murder Cases ")
```

## Problem 3

#### Write a function to generate 5000 datasets from the model, while at the same time save mu hat and p-value.

```{r}
sim_stat <- function(mu, sigma, n){
  output <- vector("list", 5000)
  i = 1
  while(i <= 5000){
    sim_data = rnorm(n, mean = mu, sd = sigma)
    
    sim_stats = t.test(sim_data, mu = 0, conf.level = 0.95) %>% 
    tidy() %>%
    select(estimate, p.value)
    
    output[[i]] = sim_stats
    
    i = i+1
  }

  bind_rows(output)
}
```

#### Generate 5000 datasets from the model mu = 0, sigma = 5, n = 30, and get the estimate and stats from each test

```{r}
sim_stat(0,5,30)
```



#### Repeat the above for $\mu = \{1,2,3,4,5,6\}$, and we make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of mu hat on the x axis.


```{r}
power_df <- data.frame()

for(i in 0:6) {
  power_df[i+1,1] = i
  power_df[i+1,2] = pull(sim_stat(i,5,30) %>% filter(p.value <= 0.05) %>% count()/5000)
}
```

#### Make a plot of power and the true value of mu.

```{r}
power_df %>% transmute(true_mu = V1, power = V2) %>%
  mutate(true_mu = as.numeric(true_mu)) %>%
  ggplot(aes(x = true_mu, y = power)) +
  geom_point(aes(color = true_mu, size = 3)) +
  geom_smooth(se = FALSE) + 
  labs(
    x = "True Mu",
    y = "Power",
    title = "True Mu vs Power"
  )
  
``` 

From the plot, we can see that when the effect size increases, the power will increase and will be eventually close to 1.

#### Make a plot showing the average estimate of $\hat{\mu}$ on the y axis and the true value of $\mu$ on the x axis and ake a second plot (or overlay on the first) the average estimate of $\hat{\mu}$ only in samples for which the null was rejected on the y axis and the true value of $\mu$ on the x axis.

```{r}
average_df <- data.frame()

for(i in 0:6) {
  average_df[i+1,1] = i
  average_df[i+1,2] = sim_stat(i,5,30) %>% select(estimate) %>% colMeans()
  average_df[i+1,3] = sim_stat(i,5,30) %>% filter(p.value <= 0.05) %>% select(estimate) %>% colMeans()
}

```
```{r}
average_df %>%
  transmute(true_mu = V1, estimate_mean = V2, reject_mean = V3) %>%
  pivot_longer(2:3, values_to = "mean_value", names_to = "mean_type") %>%
  ggplot() +
  geom_point(mapping = aes(x = true_mu, y = mean_value, color = mean_type)) +
  geom_smooth(mapping = aes(x = true_mu, y = mean_value, color = mean_type), se = FALSE) +
  labs(
    x = "True Mu",
    y = "Estimate of Mu",
    title = "True Mu vs Power"
  )
```

The true mean is approximately equal to the average estimate for all samples. However, in the plot of mean estimate for the rejected sample, the average estimate only approximates the true mean when the effect size is between 4 and 6. This shows that when effect size is large, the average estimate is a good approximation of the true value, and when the effect size is small, the the average estimate is not a good approximation of the true value.
